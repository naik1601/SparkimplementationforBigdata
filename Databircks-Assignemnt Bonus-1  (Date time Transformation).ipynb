{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6987a7a6-3052-4bf1-870d-06369bbbc824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"/Volumes/workspace/default/demo/server_logs.csv\",header=True)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481ac1e5-5d14-4b45-be93-b3e62bff919d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:red\">\n",
    "<b>Input Data set is attached in the assignment get the file from there.<br>\n",
    "   \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b3ac98b-ae08-42ca-83aa-2c68621fd1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Convert timestamp from string to DateTime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab3fead4-1dbe-4381-8bdf-5cec62470790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "df1 = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ceaa11f-bcb6-4659-a6bc-d4c4983acfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Extract the date from timestamp and add it as a new column date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8dccf8-21cf-4e27-9e29-6874cef7cc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import to_date, col\n",
    "df2 = df1.withColumn(\"Date\", to_date(col(\"timestamp\")))\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "808bdc43-42d2-4945-9bf4-99ff444ee1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Find the number of logs each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d890aac-43dd-49fb-9ac2-999158e327c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col\n",
    "df3=df2.groupBy(col('Date')).count()\n",
    "df3.display()\n",
    "# Write your code using the Spark Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e18b7e-62b5-4410-b160-18f25ae651a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Extract hour from timestamp and analyze log count per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947dd3b0-5b8d-4535-a115-0f459ae8f1ab",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771200815209}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, col\n",
    "\n",
    "hourly = (\n",
    "    df.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "      .groupBy(\"hour\")\n",
    "      .count()\n",
    "      .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "display(hourly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a481d489-9076-491d-9413-afcb2f9bbb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Identify the busiest hour of the day based on log count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546e94d4-e4f9-4713-bf37-79b5c9be164c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, col\n",
    "\n",
    "busiest_hour = (\n",
    "    df.withColumn(\"hour\", hour(col(\"timestamp\")))\n",
    "      .groupBy(\"hour\")\n",
    "      .count()\n",
    "      .orderBy(col(\"count\").desc())\n",
    "      .limit(1)\n",
    "      \n",
    ")\n",
    "\n",
    "display(busiest_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0f2ec1-08df-4ebf-937d-f9917e196791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Convert timestamp to a different string format: 'YYYY/MM/DD HH:mm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dbcf66-0cea-4387-aa8b-60a88e540f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col,to_date,lit\n",
    "df7=df.withColumn('StringDate',lit('01-01-2024'))\n",
    "dateformat=\"YYYY-MM-dd\"\n",
    "df8=df7.withColumn('Date',to_date(col('StringDate'),dateformat))\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1119178d-1eb5-4308-afa1-4024d6c9e369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the average response time for each URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bddabe-f12c-4579-9f2d-e0cfe8e79022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import avg,col\n",
    "df8 = df.select(avg(col('response_time_ms')))\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a86448f9-436d-42c7-b889-52015d495ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Filter logs with response time greater than a certain threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc23260b-46d1-4f02-8c4a-a8a8c85e926b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "threshold = 100\n",
    "df_slow = df.filter(col(\"response_time_ms\").cast(\"double\") > threshold)\n",
    "\n",
    "display(df_slow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f884bb3b-36a4-4ef9-9a89-634eff8ee781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Find the top 5 most visited URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297aa8ea-7ffa-4db0-90b3-ad73ecb512bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col\n",
    "df9=df.withColumn('Top5url', col('url')).groupBy(col('url')).count().orderBy(col('count').desc()).limit(5)\n",
    "df9.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c22077b-bf59-4c3d-bc9e-fca18a0873c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the day of the week for each log and count logs per day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b521f80e-a049-4533-a328-45694ce5f289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "df_dayofweek = df.withColumn(\"day_of_week\", date_format(col(\"timestamp\"), \"EEEE\"))\n",
    "logs_per_dayofweek = df_dayofweek.groupBy(\"day_of_week\").count().orderBy(\"day_of_week\")\n",
    "\n",
    "display(logs_per_dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8de25cdb-8bee-4652-bebb-73a9549771ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a6d437e-7233-44c7-93be-c3f86dfcd55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Identify logs with empty or null timestamp and count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be214a7a-5845-45ee-925b-3df79bc67bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_null_empty = df.filter((col('timestamp').isNull()) | (col('timestamp') == \"\"))\n",
    "count_null_empty = df_null_empty.count()\n",
    "display(df_null_empty)\n",
    "print(count_null_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1befcd43-2d94-4a98-a9d6-660c8f301948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col,when\n",
    "df10=df.filter(col('timestamp').isNull()).count()\n",
    "print(df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315bf294-cbea-4284-be44-57d5132069f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Split user_agent into browser name and version and add as new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e1a535-bf47-4ae0-8306-4083a68a2b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col,split\n",
    "df11=df.withColumn('Newcolumn',split(col('user_agent'),'/'))\n",
    "df11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b87cf7d-fde1-44f0-98c5-e84e320677c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Find the first and last log entry for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b7df74-011a-44d8-9e2c-6da30dd677e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, min as _min, max as _max\n",
    "\n",
    "df_day = df.withColumn(\"day\", to_date(col(\"timestamp\")))\n",
    "\n",
    "first_last = (\n",
    "    df_day.groupBy(\"day\")\n",
    "          .agg(\n",
    "              _min(col(\"timestamp\")).alias(\"first_ts\"),\n",
    "              _max(col(\"timestamp\")).alias(\"last_ts\")\n",
    "          ).limit(1)\n",
    "          .orderBy(\"day\")\n",
    ")\n",
    "\n",
    "display(first_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e790a6b-6394-4ba0-90e2-37141ae90d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c93dbe-67e5-460e-bab0-804b533ff977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Extract the month from timestamp and analyze log count per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843bafd2-3343-4ccd-823d-4d9e90b2190c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "df_month = df.withColumn(\"month\", date_format(col(\"timestamp\"), \"MMMM\"))\n",
    "logs_per_month = df_month.groupBy(\"month\").count().orderBy(\"month\")\n",
    "\n",
    "display(logs_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847bc2e8-bb32-48aa-8fa6-2ab8f55cea80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5e9e2a9-4133-407e-bc52-c332538b3a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Determine the average response time per hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f67bd07c-76c9-44d1-9a64-1d7db068fc7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import to_date,date_format,avg\n",
    "df13=df.withColumn('avg',date_format(col('timestamp'),\"hh\"))\n",
    "df14=df13.select(avg(col('avg')))\n",
    "df14.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625e6480-0d48-428e-8bc5-23c64431af3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Create a new column showing the time of day (Morning, Afternoon, Evening, Night) based on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb7bb1b-d5a7-4922-bcd7-22977a531f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, col, when\n",
    "\n",
    "df_time_of_day = df.withColumn(\n",
    "    \"time_of_day\",\n",
    "    when(hour(col(\"timestamp\")).between(5, 11), \"Morning\")\n",
    "    .when(hour(col(\"timestamp\")).between(12, 16), \"Afternoon\")\n",
    "    .when(hour(col(\"timestamp\")).between(17, 20), \"Evening\")\n",
    "    .otherwise(\"Night\")\n",
    ")\n",
    "\n",
    "display(df_time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3296515c-2b05-4f9d-96c9-c9ea595044d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b3c7eb8-65ec-47d1-94e9-80d9e4bc5f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Count the number of logs for each user_agent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82b5d00-7e44-4076-8080-826d0c11ffb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col\n",
    "df15=df.withColumn('Numberoflogs',col('user_agent')).groupBy('user_agent').count()\n",
    "df15.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31cc824e-183d-4cf1-ad6b-71126c3fa92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Find logs where the url contains a specific keyword (e.g., 'contact')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36e0b45-f426-4870-bb83-8200e4491f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function\n",
    "from pyspark.sql.functions import col\n",
    "df15=df.filter(col(\"url\")=='/contact')\n",
    "df15.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a0ab65-4e44-4edd-8781-3c2d9723c9fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Convert the timestamp to a Unix timestamp format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0334b3-8c1e-4be3-8e8a-853b5df97893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "\n",
    "df_unix = df.withColumn(\"unix_ts\", unix_timestamp(col(\"timestamp\")))\n",
    "display(df_unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d697b9a0-d6e2-425b-a66b-7ee96e56f7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec198c70-feec-41b0-8c9d-510bf76d87ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Calculate the time difference in minutes between consecutive logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bf63ed-6348-44b7-a8e0-6140a40d7d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_ts = df.withColumn(\"timestamp_ts\", to_timestamp(col(\"timestamp\")))  # add format string if needed\n",
    "\n",
    "w = Window.orderBy(col(\"timestamp_ts\"))\n",
    "\n",
    "df2 = (\n",
    "    df_ts.withColumn(\"prev_ts\", lag(col(\"timestamp_ts\")).over(w))\n",
    "         .withColumn(\n",
    "             \"diff_minutes\",\n",
    "             (col(\"timestamp_ts\").cast(\"long\") - col(\"prev_ts\").cast(\"long\")) / 60\n",
    "         )\n",
    ")\n",
    "\n",
    "display(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b484f9ac-b724-4f7a-975f-cebe0fd73201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code using the Spark Function"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databircks-Assignemnt Bonus-1  (Date time Transformation)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
