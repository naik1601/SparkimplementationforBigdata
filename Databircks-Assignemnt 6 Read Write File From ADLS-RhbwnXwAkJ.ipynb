{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df7e83ba-34cd-4f9e-8fb5-eb5ec719cf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<span style=\"color:orange\">\n",
    " <h2> Databircks-Assignemnt 6 Read Write File From ADLS\n",
    "</span>\n",
    "  <h5>\n",
    "    <span style=\"color:red\">\n",
    "<b>Author: Deepak Goyal <br>\n",
    "   <a> adeus.azurelib.com </a><br>\n",
    "   Email at: admin@azurelib.com\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c53c84-5d19-4115-a775-314575ce42fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Upload the Employee.csv under the ADLS storage account raw container. Load the file as dataframe in the databricks from ADLS.  Remvove all the employees which has emp_id as null. Save the result as parquet file in the ADLS location '/mnt/azurelib/processed/Employee' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4e6162-dd1d-497a-b403-121b7b3ad8da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "501c0773-29e0-4874-a318-40061bbd6390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Upload the Employee.csv under the ADLS storage account raw container. Load the file as dataframe in the databricks from ADLS.  Remvove all the employees which has emp_id as null. Save the result as CSV file in the ADLS location '/mnt/azurelib/processed/Employee_CSV' <br>\n",
    "  Also make sure to write the first line as header within the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa98591e-264d-40de-98b3-06216b930e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5808f8d9-c090-44bb-bb10-656148bf500f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Upload the Employee.csv under the ADLS storage account raw container. Load the file as dataframe in the databricks from ADLS.  Remvove all the employees which has emp_id as null. Save the result as JSON file in the ADLS location '/mnt/azurelib/processed/Employee_JSON' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b38960-360b-4712-bd45-e49649af3d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f1353f7-166f-4df7-ac6d-5861c728d679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b>Load the Employee csv file as dataframe in the databricks from ADLS again. This time Remvove all the employees which has first_name as null or empty. Save the result as parquet file in the ADLS location '/mnt/azurelib/processed/Employee'. But while saving the result ensure that older data shouldn't be overwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5568b7c2-6562-4ed4-884d-1d1d77f0a05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23e60ce-7a6e-4a66-9802-d5be8e27adfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Upload the Employee.csv under the ADLS storage account raw container. Load the file as dataframe in the databricks from ADLS.  Remvove all the employees which has emp_id as null and salary less than 10,000. Save the result as JSON file in the ADLS location '/mnt/azurelib/processed/Employee_JSON' But while saving the result ensure that if there is any old data availble it should overwritten. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41254d4c-040e-4087-8cbd-c3fa0b1186f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9a4548-cd53-477e-9b5e-201727a9a0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<b> Upload the Employee.csv under the ADLS storage account raw container. Load the file as dataframe in the databricks from ADLS.  Find out all the male employees. Save the result as CSV file in the ADLS location '/mnt/azurelib/processed/Employee_CSV' But ensure that if the folder already exist at the time of writting then it should throw out error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4288320d-f41e-4719-99a6-93fe790beacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databircks-Assignemnt 6 Read Write File From ADLS-RhbwnXwAkJ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
